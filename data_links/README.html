<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>in</title>
  <style>
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
    /* CSS for syntax highlighting */
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { color: #008000; } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { color: #008000; font-weight: bold; } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<h1>ICCV 2023 HoloAssist: an egocentric human interaction dataset for
interactive ai assistants in the real world</h1>
<p>The codebase provides guidelines for using the HoloAssist dataset and
running the benchmarks.</p>
<p>[<a href="https://holoassist.github.io/">Project Website</a>][<a
href="https://openaccess.thecvf.com/content/ICCV2023/html/Wang_HoloAssist_an_Egocentric_Human_Interaction_Dataset_for_Interactive_AI_Assistants_ICCV_2023_paper.html">paper</a>][<a
href="#download-the-data-and-annotations">data</a>]</p>
<h1>Download the data and annotations</h1>
<p>We release the dataset under the [<a
href="https://cdla.dev/permissive-2-0/">CDLAv2</a>] license, a
permissive license. You can download the data and annotations via the
links in the text files below. You can either downloading the data
through your web browser or using [<a
href="https://learn.microsoft.com/en-us/azure/storage/common/storage-use-azcopy-v10">Azcopy</a>]
which will be <b>faster</b>.</p>
<p>Data links:</p>
<ul>
<li><a
href="https://holoassist.github.io/data_links/video_pitch_shifted.txt">Video
(pitch shifted, 184.20 GB)</a></li>
<li><a
href="https://holoassist.github.io/data_links/compressed_video.txt">Compressed
Videos (resize with width to 256, 144.62 GB)</a></li>
<li><a
href="https://holoassist.github.io/data_links/ahat_depth.txt">Ahat Depth
(560.46 GB)</a></li>
<li><a href="https://holoassist.github.io/data_links/eye_gaze.txt">Eye
Gaze (2.45 GB)</a></li>
<li><a href="https://holoassist.github.io/data_links/hands.txt">Hand
pose (219.24 GB)</a></li>
<li><a href="https://holoassist.github.io/data_links/head.txt">Head pose
(4.67 GB)</a></li>
<li><a href="https://holoassist.github.io/data_links/imu.txt">IMU (4.63
GB)</a></li>
</ul>
<p>Annotation links:</p>
<ul>
<li><a href="">Annotation</a></li>
</ul>
<p><strong>Install Azcopy and download data via Azcopy in
Linux.</strong></p>
<p>Please refer to the <a
href="https://learn.microsoft.com/en-us/azure/storage/common/storage-use-azcopy-v10">official
manual</a> of using Azcopy in other OS.</p>
<div class="sourceCode" id="cb1"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="ex">-</span> wget <span class="at">-O</span> azcopy.tar.gz https://aka.ms/downloadazcopy-v10-linux</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="ex">-</span> tar <span class="at">-xvf</span> azcopy.tar.gz</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="ex">-</span> sudo mv azcopy_linux_amd64_<span class="pp">*</span>/azcopy /usr/bin</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="ex">-</span> azcopy <span class="at">--version</span></span></code></pre></div>
<p>Downloading the data</p>
<div class="sourceCode" id="cb2"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="ex">-</span> azcopy copy <span class="st">&quot;&lt;data_url&gt;&quot;</span> <span class="st">&quot;&lt;local_directory&gt;&quot;</span> <span class="at">--recursive</span></span></code></pre></div>
<h2>Dataset Structure</h2>
<p>Once the dataset is downloaded and decompressed. You will see the
dataset structure as follows. Each folder contains data for one
recording session. Within each folder, you will see the data for
different modalities. The text files with "_synced" in the names are
synced according to the RGB modality as each modality has different
sensor rate and we use the synced modalities in the experiments.</p>
<p>We collected our dataset using <a
href="https://github.com/microsoft/psi/tree/master/Sources/MixedReality/HoloLensCapture">PSI
studio</a>. More detailed information regarding the data format is in <a
href="https://github.com/microsoft/psi/tree/master/Sources/MixedReality/HoloLensCapture/HoloLensCaptureExporter">here</a>.</p>
<pre>
  .
  ├── R007-7July-DSLR
  │   └── Export_py
  │       │── AhatDepth
  │       │   ├── 000000.png
  │       │   ├── 000001.png
  │       │   ├── ...
  │       │   ├── AhatDepth_synced.txt
  │       │   ├── Instrinsics.txt
  │       │   ├── Pose_sync.txt
  │       │   └── Timing_sync.txt
  │       ├── Eyes
  │       │   └── Eyes_sync.txt 
  │       ├── Hands
  │       │   ├── Left_sync.txt
  │       │   └── Right_sync.txt 
  │       ├── Head
  │       │   └── Head_sync.txt 
  │       ├── IMU
  │       │   ├── Accelerometer_sync.txt
  │       │   ├── Gyroscope_sync.txt
  │       │   └── Magnetometer_sync.txt
  │       ├── Video
  │       │   ├── Pose_sync.txt
  │       │   ├── Instrinsincs.txt
  │       │   └── VideoMp4Timing.txt
  │       ├── Video_pitchshift.mp4
  │       └── Video_compress.mp4
  ├── R012-7July-Nespresso/
  ├── R013-7July-Nespresso/
  ├── R014-7July-DSLR/
  └── ...
</pre>

<h2>Annotation Structure</h2>
<p>We have released both the annotations in the raw format and the
processed format. We also provide the <a
href="src/data_2222/train.txt">train</a>, <a
href="src/data_2222/val.txt">validation</a> and <a
href="src/data_2222/test.txt">test</a> splits.</p>
<p>In the raw annotations, each annotation follows</p>
<pre>
{
    "id": int, original label id,
    "label": "Narration", "Conversation", "Fine grained action",  or "Coarse grained action", 
    "start": start time in seconds, 
    "end": end time in seconds, 
    "type":"range",
    "attributes":{
        Different from different label task. See below.
    },
},
</pre>

<p>Attributes for <strong>Narration</strong></p>
<pre>
    "id": int, original label id,
    "label": "Narration",  
    "start": start time in seconds, 
    "end": end time in seconds, 
    "type":"range",
    "attributes": {
        "Long-form description": Use multiple sentences and make this as long as is necessary to be exhaustive. There are a finite number of scenarios across all videos, so make sure to call out the distinctive changes between videos, in particular, mistakes that the task performer makes in the learning process that are either self-corrected or corrected by the instructor.
    }, 
</pre>

<ul>
<li><strong>Example</strong>: A man operates a big office printer. The
instructor provides directions on how to turn on and load paper into the
big office printer. The man turns on the printer and then turns it off.
He then loads paper into the first drawer of the printer and replaces
the first black cartridge from left to right in the printer. The
instructor corrects the man on where to place the first black cartridge.
The man moves the black cartridge from its current position to the
correct location. Note: The time stamps for this annotation will always
start at 0 and end at the end of the video.</li>
</ul>
<p>Attributes for <strong>Conversation</strong></p>
<pre>
    "id": int, original label id,
    "label": "Narration",  
    "start": start time in seconds, 
    "end": end time in seconds, 
    "type":"range",
    "attributes": {
        "Conversation Purpose":"instructor-start-conversation_other",
        "Transcription":"*unintelligible*",
        "Transcription Confidence":"low-confidence-transcription",
    }, 
</pre>

<ul>
<li><p><strong>Conversation Purpose</strong>: Select an option that best
describes the purpose of the speech. This is limited to the individual
speaking and does not include any pause time waiting for a response.</p>
<ul>
<li>Instructor-start-conversation: Describing high-level
instruction</li>
<li>Instructor-start-conversation: Opening remarks</li>
<li>Instructor-start-conversation: Closing remarks</li>
<li>Instructor-start-conversation: Adjusting to capture better quality
video</li>
<li>Instructor-start-conversation: Confirming the previous or future
action</li>
<li>Instructor-start-conversation: Correct the wrong action</li>
<li>Instructor-start-conversation: Follow-up instruction</li>
<li>Instructor-start-conversation: Other</li>
<li>Instructor-reply-to-task performer: Confirming the previous or
future action</li>
<li>Instructor-reply-to-task performer: Correct the wrong action</li>
<li>Instructor-reply-to-task performer: Follow-up instruction</li>
<li>Instructor-reply-to-task performer: other</li>
<li>task performer-start-conversation: ask questions</li>
<li>task performer-start-conversation: others</li>
</ul></li>
<li><p><strong>Transcriptions</strong>: Transcribe the conversation into
texts.</p></li>
<li><p><strong>Transcription Confidence</strong>: Confidence for the
human annotator at translating the speech to text.</p></li>
</ul>
<p>Attributes for <strong>Fine grained action</strong></p>
<pre>
    "id": int, original label id,
    "label": "Fine grained action",  
    "start": start time in seconds, 
    "end": end time in seconds, 
    "type":"range",
    "attributes": {
        "Action Correctness":"Correct Action",
        "Incorrect Action Explanation":"none",
        "Incorrect Action Corrected by":"none",
        "Verb":"approach",
        "Adjective":"none",
        "Noun":"gopro",
        "adverbial":"none"
    }, 
</pre>

<ul>
<li><p><strong>Action Correctness</strong>: Indicate whether the action
is correct or a mistake to achieve the task. The options are</p>
<ul>
<li>Correct action</li>
<li>Wrong action, corrected by instructor verbally</li>
<li>Wrong action, corrected by performer</li>
<li>Wrong action, not corrected</li>
<li>Others</li>
</ul></li>
<li><p><strong>Incorrect Action Explanation</strong>: Provided by the
human annotators to explain why the believe the action is
wrong.</p></li>
<li><p><strong>Incorrect Action Correct by</strong>: Indicate whether
the wrong action is later corrected by the instructor or the task
performer.</p></li>
<li><p><strong>Verb</strong>, <strong>Adjective</strong>,
<strong>Noun</strong>, <strong>adverbial</strong>: Verb, (adjective),
Noun, (adverbial) for describing the fine-grained actions.</p></li>
</ul>
<p>Attributes for <strong>Coarse grained action</strong></p>
<pre>
    "id": int, original label id,
    "label": "Coarse grained action",  
    "start": start time in seconds, 
    "end": end time in seconds, 
    "type":"range",
    "attributes": {
        "Action sentence":"The student changes the battery for the GoPro.",
        "Verb":"exchange",
        "Adjective":"none",
        "Noun":"battery"
    }, 
</pre>

<ul>
<li><strong>Action sentence</strong>: A factual statement describing the
interaction that the collector/camera wearer is performing with a
digital device and the software on the device.
<ul>
<li>Example 1: A man changes the battery of the bright green GoPro.</li>
<li>Example 2: A woman attaches the leg to a chair.</li>
</ul></li>
<li><strong>Verb</strong>: This verb was part of the Coarse-Grained
Action sentence.
<ul>
<li>Example 1: Change</li>
<li>Example 2: Attach</li>
</ul></li>
<li><strong>Adjective</strong>: This is the adjective(s) that helps
distinguish the noun from other similar items. This field is optional if
the noun is unique enough on its own.
<ul>
<li>Example 1: bright green</li>
<li>Example 2: [blank]</li>
</ul></li>
<li><strong>Noun</strong>: This is the generic noun that is part of the
Coarse-Grained Action sentence.
<ul>
<li>Example 1: GoPro</li>
<li>Example 2: Leg</li>
</ul></li>
</ul>
<p>To convert the raw annotation into the format we used in the
benchmark experiments, you can either run the <a
href="src/data_2221/process_labels.py">label processing script</a> or
use our processed the labels in the <a
href="#download-the-data-and-annotations">links</a> above.</p>
<p><strong>Citation</strong></p>
<p>If you find the code or data useful. Please consider cite the paper
at</p>
<pre>@inproceedings{wang2023holoassist,
  title={Holoassist: an egocentric human interaction dataset for interactive ai assistants in the real world},
  author={Wang, Xin and Kwon, Taein and Rad, Mahdi and Pan, Bowen and Chakraborty, Ishani and Andrist, Sean and Bohus, Dan and Feniello, Ashley and Tekin, Bugra and Frujeri, Felipe Vieira and others},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={20270--20281},
  year={2023}
}</pre>
</body>
</html>
